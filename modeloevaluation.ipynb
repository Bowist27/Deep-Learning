# =============================================
# FF++ C23 ‚Üí Deepfake 7 clases (original + 6 m√©todos)
# ENTRENAMIENTO desde carpetas ya divididas (train/val/test)
# EfficientNetB0 + RandomJPEG FIX + label_smoothing (one-hot)
# TensorFlow 2.18 / Keras 3
# SIN LAMBDA LAYERS - Todo con capas custom serializables
# =============================================

# ---------- 0) Sistema / Paquetes ----------
!pip -q install -U "tensorflow==2.18.0" "numpy==1.26.4" "pandas==2.2.2" \
                 "scikit-learn==1.5.2" "matplotlib==3.9.2" "tqdm"

# ---------- 1) Imports / Constantes ----------
from pathlib import Path
import os, sys, json, math, random, gc
import numpy as np
import pandas as pd
from tqdm.auto import tqdm

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

print("Python:", sys.version)
print("TF:", tf.__version__)
print("GPU:", tf.config.list_physical_devices('GPU'))

# Rutas
DATA_ROOT = Path("/content/data_faces_ffpp")   # <-- AJUSTA SI ES NECESARIO
CKPT_DIR  = Path("/content/ffpp_checkpoints")
CKPT_DIR.mkdir(parents=True, exist_ok=True)

# Clases (orden fijo, deben coincidir con nombres de carpetas)
METHODS = [
    "original",
    "DeepFakeDetection",
    "Deepfakes",
    "Face2Face",
    "FaceShifter",
    "FaceSwap",
    "NeuralTextures",
]
NUM_CLASSES = len(METHODS)

# Entrenamiento
IMG_SIZE = (224, 224)
BATCH    = 32  # Reducido de 64 para evitar OOM
AUTOTUNE = tf.data.AUTOTUNE
SEED     = 42

random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

# ---------- 1.1) CAPAS CUSTOM SERIALIZABLES ----------
# IMPORTANTE: En Keras 3 con TF 2.18, importar desde keras directamente
from keras.saving import register_keras_serializable

@register_keras_serializable(package="Custom", name="RandomJPEG")
class RandomJPEG(layers.Layer):
    """Compresi√≥n JPEG aleatoria por imagen. Espera float32 [0..1]; devuelve [0..1]."""
    def __init__(self, qmin=60, qmax=95, **kwargs):
        super().__init__(**kwargs)
        self.qmin = int(qmin)
        self.qmax = int(qmax)

    def call(self, x, training=None):
        if training is False:
            return x
        x8 = tf.cast(tf.clip_by_value(x * 255.0, 0.0, 255.0), tf.uint8)
        def _per_image(img):
            return tf.image.random_jpeg_quality(img, self.qmin, self.qmax)
        x2 = tf.map_fn(_per_image, x8, fn_output_signature=tf.uint8)
        return tf.cast(x2, tf.float32) / 255.0

    def compute_output_shape(self, input_shape):
        return input_shape

    def get_config(self):
        cfg = super().get_config()
        cfg.update({"qmin": self.qmin, "qmax": self.qmax})
        return cfg


@register_keras_serializable(package="Custom", name="ScaleTo255")
class ScaleTo255(layers.Layer):
    """Convierte de rango [0,1] a [0,255]."""
    def call(self, x):
        return x * 255.0

    def compute_output_shape(self, input_shape):
        return input_shape


@register_keras_serializable(package="Custom", name="EfficientNetPreprocess")
class EfficientNetPreprocess(layers.Layer):
    """Preprocesamiento espec√≠fico de EfficientNet (normalizaci√≥n ImageNet)."""
    def call(self, x):
        # EfficientNet preprocess: (x - mean) / std donde mean=[123.675, 116.28, 103.53]
        # Implementaci√≥n manual para evitar importar funci√≥n externa
        return tf.keras.applications.efficientnet.preprocess_input(x)

    def compute_output_shape(self, input_shape):
        return input_shape


# ---------- 2) Utilidades ----------
def _count_images(p: Path, ext=(".png", ".jpg", ".jpeg", ".bmp", ".webp")):
    if not p.exists(): 
        return 0
    return sum(1 for q in p.rglob("*") if q.is_file() and q.suffix.lower() in ext)


def sanity_check_structure(root: Path):
    ok = True
    for split in ["train", "val", "test"]:
        sp_dir = root / split
        if not sp_dir.exists():
            print(f"[WARN] Falta carpeta: {sp_dir}")
            ok = False
        else:
            for cls in METHODS:
                cls_dir = sp_dir / cls
                n = _count_images(cls_dir)
                print(f"{split:>5}/{cls:<18} ‚Üí {n} imgs")
                if n == 0: 
                    print(f"[WARN] {cls_dir} sin im√°genes")
    return ok


print("Revisando estructura de carpetas...")
sanity_check_structure(DATA_ROOT)

# ---------- 3) tf.data loaders (ONE-HOT) ----------
def build_ds(split, batch=BATCH, cache=False, shuffle=True):
    ds = tf.keras.utils.image_dataset_from_directory(
        DATA_ROOT / split,
        labels="inferred",
        label_mode="categorical",      # one-hot para label_smoothing
        class_names=METHODS,           # orden fijo de clases
        image_size=IMG_SIZE,
        batch_size=batch,
        shuffle=shuffle,
        seed=SEED,
        interpolation="bilinear"
    )
    if cache: 
        ds = ds.cache()
    return ds.prefetch(AUTOTUNE)


# Solo cachear val/test si son peque√±os (<10k im√°genes cada uno)
ds_train = build_ds("train", cache=False, shuffle=True)
ds_val   = build_ds("val",   cache=False, shuffle=False)  # Cambio: cache=False
ds_test  = build_ds("test",  cache=False, shuffle=False)  # Cambio: cache=False

# ---------- 4) Pesos de clase (convertidos a sample_weight) ----------
cls_counts = {i: _count_images(DATA_ROOT / "train" / METHODS[i]) for i in range(NUM_CLASSES)}
tot = sum(cls_counts.values())
class_weight = {i: (tot / (NUM_CLASSES * max(1, cls_counts[i]))) for i in range(NUM_CLASSES)}
print("class_weight:", class_weight)

# Mapeo a sample_weight por ejemplo (robusto con one-hot)
w_vec = tf.constant([class_weight[i] for i in range(NUM_CLASSES)], dtype=tf.float32)

def add_sample_weights(x, y):
    # y es one-hot -> idx = argmax
    idx = tf.argmax(y, axis=1)
    sw = tf.gather(w_vec, idx)
    return x, y, sw

ds_train_w = ds_train.map(add_sample_weights, num_parallel_calls=AUTOTUNE)

# ---------- 5) Modelo EfficientNetB0 (SIN LAMBDA) ----------
from tensorflow.keras.applications import EfficientNetB0

def build_model_efficientnetb0():
    inp = layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))
    
    # Augmentaci√≥n + preprocesamiento (todo con capas custom)
    x = layers.Rescaling(1./255)(inp)             # [0,1]
    x = layers.RandomFlip("horizontal")(x)
    x = layers.RandomRotation(0.05)(x)
    x = layers.RandomZoom(0.1)(x)
    x = RandomJPEG(60, 95)(x)                     # [0,1]
    x = ScaleTo255()(x)                           # [0,255] - CUSTOM LAYER
    x = EfficientNetPreprocess()(x)               # Normalizaci√≥n - CUSTOM LAYER
    
    # Backbone preentrenado
    base = EfficientNetB0(include_top=False, weights="imagenet")
    base.trainable = False                        # warmup congelado
    y = base(x, training=False)
    
    # Head de clasificaci√≥n
    y = layers.GlobalAveragePooling2D()(y)
    y = layers.Dropout(0.30)(y)
    out = layers.Dense(NUM_CLASSES, activation="softmax", name="out")(y)
    
    model = keras.Model(inp, out)
    return model, base


model, base = build_model_efficientnetb0()

# P√©rdida y m√©tricas para ONE-HOT
LOSS = keras.losses.CategoricalCrossentropy(label_smoothing=0.05)
METRICS = [
    keras.metrics.CategoricalAccuracy(name="acc"),
    keras.metrics.TopKCategoricalAccuracy(k=3, name="top3"),
]

model.compile(
    optimizer=keras.optimizers.Adam(1e-3), 
    loss=LOSS, 
    metrics=METRICS
)
model.summary()

# ---------- 6) Callbacks ----------
ckpt_every_epoch = str(CKPT_DIR / "weights_epoch_{epoch:03d}.weights.h5")
ckpt_best        = str(CKPT_DIR / "best_by_valACC.keras")
csv_log          = str(CKPT_DIR / "training_log.csv")

cbs = [
    keras.callbacks.ModelCheckpoint(
        filepath=ckpt_every_epoch, 
        save_weights_only=True, 
        save_freq="epoch", 
        verbose=1
    ),
    keras.callbacks.ModelCheckpoint(
        filepath=ckpt_best, 
        save_weights_only=False, 
        monitor="val_acc", 
        mode="max",
        save_best_only=True, 
        verbose=1
    ),
    keras.callbacks.ReduceLROnPlateau(
        monitor="val_acc", 
        factor=0.5, 
        patience=2, 
        min_lr=1e-6, 
        verbose=1
    ),
    keras.callbacks.EarlyStopping(
        monitor="val_acc", 
        mode="max", 
        patience=4, 
        restore_best_weights=True, 
        verbose=1
    ),
    keras.callbacks.CSVLogger(csv_log, append=True),
    keras.callbacks.BackupAndRestore(backup_dir=str(CKPT_DIR / "backup"))  # NUEVO: backup autom√°tico
]

# ---------- 7) Entrenamiento: fase 1 (warmup) ----------
print("\n" + "="*60)
print("FASE 1: WARMUP (backbone congelado)")
print("="*60)

WARMUP_EPOCHS = 5
history1 = model.fit(
    ds_train_w, 
    validation_data=ds_val,
    epochs=WARMUP_EPOCHS,
    callbacks=cbs,
    verbose=1
)

# Limpieza de memoria
gc.collect()
tf.keras.backend.clear_session()

# ---------- 8) Fine-tune: descongela ~50% final ----------
print("\n" + "="*60)
print("FASE 2: FINE-TUNING (50% del backbone descongelado)")
print("="*60)

n_layers = len(base.layers)
for li, lyr in enumerate(base.layers):
    lyr.trainable = (li >= n_layers // 2)

model.compile(
    optimizer=keras.optimizers.Adam(1e-5),  # LR bajo para FT
    loss=LOSS,
    metrics=METRICS,
)

FINE_EPOCHS = 10
history2 = model.fit(
    ds_train_w, 
    validation_data=ds_val,
    epochs=FINE_EPOCHS,
    callbacks=cbs,
    verbose=1
)

# Limpieza de memoria
gc.collect()

# ---------- 9) Cargar mejor modelo (AHORA ES SEGURO, sin Lambda) ----------
print("\n" + "="*60)
print("CARGANDO MEJOR MODELO")
print("="*60)

best_model = keras.models.load_model(
    ckpt_best,
    custom_objects={
        "RandomJPEG": RandomJPEG,
        "ScaleTo255": ScaleTo255,
        "EfficientNetPreprocess": EfficientNetPreprocess
    },
    compile=False
    # safe_mode ya no es necesario especificarlo
)
print("‚úì Cargado mejor modelo:", ckpt_best)

# ---------- 10) Evaluaci√≥n en test ----------
print("\n" + "="*60)
print("EVALUACI√ìN EN TEST SET")
print("="*60)

y_true_int, y_pred_int, y_prob = [], [], []
for bx, by in tqdm(ds_test, desc="Prediciendo"):
    p = best_model.predict(bx, verbose=0)             # (B, NUM_CLASSES)
    y_prob.append(p)
    y_true_int.append(np.argmax(by.numpy(), axis=1))  # ints
    y_pred_int.append(np.argmax(p, axis=1))           # ints

y_true_int = np.concatenate(y_true_int)
y_pred_int = np.concatenate(y_pred_int)
y_prob = np.concatenate(y_prob)

# Reporte por clase
report = classification_report(
    y_true_int, 
    y_pred_int, 
    target_names=METHODS, 
    output_dict=True, 
    zero_division=0
)
df_report = pd.DataFrame(report).transpose()
df_report.to_csv(str(CKPT_DIR / "classification_report_test.csv"))
print("\n" + df_report.to_string())

# Matriz de confusi√≥n
cm = confusion_matrix(y_true_int, y_pred_int, labels=list(range(NUM_CLASSES)))
fig = plt.figure(figsize=(9, 8))
plt.imshow(cm, interpolation='nearest', cmap='Blues')
plt.title("Confusion Matrix (test)", fontsize=14, pad=20)
plt.colorbar()
ticks = np.arange(NUM_CLASSES)
plt.xticks(ticks, METHODS, rotation=45, ha='right')
plt.yticks(ticks, METHODS)
plt.xlabel("Predicted", fontsize=12)
plt.ylabel("True", fontsize=12)
plt.tight_layout()
plt.savefig(str(CKPT_DIR / "confusion_matrix_test.png"), dpi=150, bbox_inches='tight')
plt.show()

# AUC one-vs-rest (macro/weighted)
try:
    from sklearn.preprocessing import label_binarize
    y_bin = label_binarize(y_true_int, classes=list(range(NUM_CLASSES)))
    auc_macro = roc_auc_score(y_bin, y_prob, average="macro", multi_class="ovr")
    auc_weighted = roc_auc_score(y_bin, y_prob, average="weighted", multi_class="ovr")
    print(f"\n‚úì ROC-AUC macro (ovr): {auc_macro:.4f} | weighted: {auc_weighted:.4f}")
except Exception as e:
    print(f"\n‚úó AUC OVR no calculable: {e}")

# ---------- 11) Exportar artefactos ----------
print("\n" + "="*60)
print("EXPORTANDO MODELOS FINALES")
print("="*60)

# 11.1) Guardar el mejor modelo tal cual (incluye augmentation)
best_path = str(CKPT_DIR / "best_final.keras")
best_model.save(best_path)
print(f"‚úì Modelo con augmentaci√≥n guardado: {best_path}")

# 11.2) Export de INFERENCIA sin augment (SIN CUSTOM LAYERS DE AUGMENT)
def export_inference(trained_model, num_classes, export_path):
    """Extrae solo backbone + head, sin augmentaci√≥n."""
    base = trained_model.get_layer("efficientnetb0")
    head_w = trained_model.get_layer("out").get_weights()
    
    # Modelo limpio de inferencia
    inp = keras.Input(shape=(224, 224, 3))
    x = EfficientNetPreprocess()(inp)  # Solo preprocess, sin augment
    y = base(x, training=False)
    y = layers.GlobalAveragePooling2D()(y)
    logits = layers.Dense(num_classes, activation="softmax", name="out")(y)
    
    inf = keras.Model(inp, logits)
    inf.get_layer("out").set_weights(head_w)
    inf.save(export_path)
    return inf

inf_path = str(CKPT_DIR / "best_inference.keras")
inference_model = export_inference(best_model, NUM_CLASSES, inf_path)
print(f"‚úì Modelo de inferencia guardado: {inf_path}")

# ---------- 12) Resumen final ----------
print("\n" + "="*60)
print("RESUMEN FINAL")
print("="*60)
print(f"üìÅ Dataset:             {DATA_ROOT}")
print(f"üìÅ Checkpoints:         {CKPT_DIR}")
print(f"üìä Reporte CSV:         {CKPT_DIR / 'classification_report_test.csv'}")
print(f"üìä Confusion matrix:    {CKPT_DIR / 'confusion_matrix_test.png'}")
print(f"üéØ Modelo completo:     {best_path}")
print(f"üöÄ Modelo inferencia:   {inf_path}")
print("="*60)
print("\n‚úÖ ENTRENAMIENTO COMPLETO - Sin errores de Lambda layers")